{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6rjRhIkoPWfv7Uyi+jJnX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JanEggers-hr/GPT-Zufalls-Experiment/blob/main/GPT_Antwort_auf_R%C3%A4tselfrage_testen.ipynb\" target=\"_parent\"><img src=\"./open-colab.gif\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wie oft löst GPT das Rätsel?\n",
        "\n",
        "Ein Skript, das GPT ein Rätsel stellt - und dann mit einer weiteren GPT-Anfrage versucht festzustellen, wie häufig die Antwort korrekt ist.\n",
        "\n",
        "Das Experiment soll zeigen, welche Rolle der Zufall in der Antwort des Sprachmodells spielt. Mehr dazu in meinem Blog: https://janeggers.tech/ - Version 1a vom 9.1.2024\n",
        "\n",
        "**Benötigt wird ein API-Token von OpenAI.**\n",
        "\n",
        "Unten beim Programmcode auf den Play-Button drücken, dann warten - und wenn alle Bibliotheken geladen sind, den Code einkopieren und Return drücken.\n",
        "\n",
        "Das Programm spricht die OpenAI-Server an - Wenn der API-Key akzeptiert wird, kommt eine entsprechende Meldung."
      ],
      "metadata": {
        "id": "1_46PotrvXDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Vorbereitung\n",
        "import requests\n",
        "import json\n",
        "import math\n",
        "import markdown\n",
        "\n",
        "# ipywidgets ist schon installiert\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Modelle und Kosten definieren\n",
        "# Kosten in US-Dollar je 1000 Tokens\n",
        "# Könnte die Modelle auch über die API holen, aber so kann ich die Kosten\n",
        "# mitgeben. Entsprechend OpenAI-Preisliste Dezember 2023. https://openai.com/pricing\n",
        "\n",
        "models_token_info = {\n",
        "          'gpt-4-1106-preview': {\n",
        "                                        'output_price': 0.03,\n",
        "                                        'input_price': 0.01,\n",
        "                                        'max_tokens': 128000\n",
        "                                      },\n",
        "          'gpt-3.5-turbo-1106': {\n",
        "                                        'output_price': 0.002,\n",
        "                                        'input_price': 0.001,\n",
        "                                        'max_tokens': 16385\n",
        "                                      },\n",
        "          'gpt-4': {\n",
        "                                        'pricing': 0.03,\n",
        "                                        'input_price': 0.01,\n",
        "                                        'max_tokens': 8192\n",
        "                                      },\n",
        "          'gpt-3.5-turbo-0613': {\n",
        "                                        'output_price': 0.002,\n",
        "                                        'input_price': 0.001,\n",
        "                                        'max_tokens': 4096\n",
        "                                      }}\n",
        "\n",
        "\n",
        "# Tokenizer Tiktoken einbinden\n",
        "!pip install -q tiktoken\n",
        "import tiktoken\n",
        "print(\"Tokenizer tiktoken geladen.\")\n",
        "\n",
        "# OpenAI-API-Library einbinden\n",
        "!pip install -q openai\n",
        "from openai import OpenAI\n",
        "print(\"OpenAI-API-Library geladen.\")\n",
        "\n",
        "##### Der eigentliche Code! #####\n",
        "\n",
        "from getpass import getpass\n",
        "from google.colab import userdata\n",
        "\n",
        "key_needed = True\n",
        "try:\n",
        "  ai_client = OpenAI(api_key = userdata.get('openai'))\n",
        "except:\n",
        "  print(\"OpenAI-Key benötigt\")\n",
        "else:\n",
        "  print(\"*** API-Key gültig! ***\")\n",
        "  key_needed = False\n",
        "\n",
        "while key_needed:\n",
        "    try:\n",
        "        # Testweise Modelle abfragen\n",
        "        ai_client = OpenAI(api_key = getpass(\"OpenAI-API-Key eingeben: \"))\n",
        "        models = ai_client.models.list()\n",
        "        # Returns a list of model objects\n",
        "        # Erfolg?\n",
        "        print()\n",
        "        print(\"*** API-Key gültig! ***\")\n",
        "        key_needed = False\n",
        "    except Exception as e:\n",
        "        print(\"Fehler bei Abfrage; ist der API-Key möglicherweise ungültig?\", e)\n",
        "\n",
        "spent_tokens = 0        # Wie viele Tokens wurden bisher über die API abgefragt?\n",
        "spent_dollars = 0.00    # Zu welchem Preis?\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "9tLhn5k9O8Xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# API-Key gültig?\n",
        "\n",
        "Jetzt auf den nächsten \"Play\"-Button klicken - die Einstell-Elemente werden erzeugt und angezeigt. Temperatur, KI-Modell, Fragetext und Anzahl der Durchgänge anpassen, wenn gewünscht - dann \"Starten\" klicken, das Experiment beginnt:\n",
        "\n",
        "* Die Rätselfrage wird der KI gestellt, so oft hintereinander wie eingestellt.\n",
        "* Eine weitere Anfrage an die KI bewertet, wie gut die Antworten waren.\n",
        "* Am Ende werden die statistischen Abweichungen bei den genannten Zahlen der Brüder und Schwestern berechnet...\n",
        "* ...und die drei häufigsten Antwort-Anfänge aufgelistet (hierbei werden nur die ersten 50 Buchstaben verglichen).\n",
        "\n",
        "Eine Tabelle der Antworten wird als Excel_Datei erzeugt und heruntergeladen.\n",
        "\n",
        "Das Rätsel stammt aus dem [LLMonitor-Benchmark](https://benchmarks.llmonitor.com/) von [@vincelwt](https://vincelwt.com); wer will, kann es gern anpassen."
      ],
      "metadata": {
        "id": "1G_mnWu9Pj3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Code für das Experiment\n",
        "\n",
        "# ipywidgets ist schon installiert\n",
        "import ipywidgets as widgets\n",
        "import requests\n",
        "from IPython.display import display\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import json\n",
        "import math\n",
        "import markdown\n",
        "\n",
        "# Modelle und Kosten definieren\n",
        "# Kosten in US-Dollar je 1000 Tokens\n",
        "models_token_info = {\n",
        "          'gpt-3.5-turbo-1106': {\n",
        "                                        'output_price': 0.002,\n",
        "                                        'input_price': 0.001,\n",
        "                                        'max_tokens': 16385\n",
        "                                      },\n",
        "          'gpt-4-1106-preview': {\n",
        "                                        'output_price': 0.03,\n",
        "                                        'input_price': 0.01,\n",
        "                                        'max_tokens': 128000\n",
        "                                      },\n",
        "          'gpt-3.5-turbo': {\n",
        "                                        'output_price': 0.002,\n",
        "                                        'input_price': 0.001,\n",
        "                                        'max_tokens': 4096\n",
        "                                      },\n",
        "}\n",
        "\n",
        "##### Der eigentliche Code! #####\n",
        "\n",
        "spent_tokens = 0        # Wie viele Tokens wurden bisher über die API abgefragt?\n",
        "spent_dollars = 0.00    # Zu welchem Preis?\n",
        "\n",
        "# Define the HTML and JavaScript code for the spinner animation\n",
        "spinner_html = \"\"\"\n",
        "<div class=\"loader\"></div>\n",
        "<style>\n",
        ".loader {\n",
        "  border: 8px solid #f3f3f3;\n",
        "  border-top: 8px solid #3498db;\n",
        "  border-radius: 50%;\n",
        "  width: 25px;\n",
        "  height: 25px;\n",
        "  animation: spin 2s linear infinite;\n",
        "  margin: 20px auto;\n",
        "}\n",
        "\n",
        "@keyframes spin {\n",
        "  0% { transform: rotate(0deg); }\n",
        "  100% { transform: rotate(360deg); }\n",
        "}\n",
        "</style>\n",
        "\"\"\"\n",
        "\n",
        "# Hilfsfunktion: Token berechnen\n",
        "def calculate_tokens(string: str) -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    # cl100k_base ist der Tokenizer für Davinci, GPT-3 und GPT-4\n",
        "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens\n",
        "\n",
        "def output_pricing(tokens):\n",
        "    price = models_token_info.get(model)['output_price']\n",
        "    # Kosten in Dollar zurückgeben\n",
        "    return(tokens * price / 1000)\n",
        "\n",
        "def input_pricing(tokens):\n",
        "    price = models_token_info.get(model)['input_price']\n",
        "    # Kosten in Dollar zurückgeben\n",
        "    return(tokens * price / 1000)\n",
        "\n",
        "def gpt(g_system,g_previous_messages,g_prompt,g_model,temperature = 0,json=False):\n",
        "    # Systemprompt und few-shots zusammenbinden\n",
        "    #\n",
        "    global max_tokens\n",
        "    global stoptokens\n",
        "    global spent_tokens\n",
        "    global spent_dollars\n",
        "    if json==True:\n",
        "        m_type = {\"type\": \"json_object\"}\n",
        "    else:\n",
        "        m_type = {\"type\": \"text\"}\n",
        "\n",
        "    prompts = [\n",
        "            {\"role\": \"system\", \"content\": g_system},\n",
        "            *g_previous_messages,\n",
        "            {\"role\": \"user\", \"content\": g_prompt},\n",
        "        ]\n",
        "    response = ai_client.chat.completions.create(\n",
        "        messages=prompts,\n",
        "        model=g_model,\n",
        "        max_tokens=max_tokens,\n",
        "        n=1,\n",
        "        response_format= m_type,\n",
        "        stream = False,\n",
        "        temperature=temperature,\n",
        "    )\n",
        "    # Anzahl verbrauchtert Tokens anpassen\n",
        "    spent_tokens += response.usage.total_tokens\n",
        "    spent_dollars += output_pricing(response.usage.completion_tokens) + input_pricing(response.usage.prompt_tokens)\n",
        "    token_usage_text = f'<b>Verbrauchte Token:</b> {spent_tokens} ($ {spent_dollars:.3f}) '\n",
        "    text_tokens.value = token_usage_text\n",
        "    return(response.choices[0].message.content)\n",
        "\n",
        "# Funktion wird bei Veränderung ausgeführt\n",
        "def update_params(change):\n",
        "    global temperature\n",
        "    global max_tokens\n",
        "    global system_prompt\n",
        "    global model\n",
        "    global stoptokens\n",
        "    global best_of\n",
        "\n",
        "# Vorbereitungen für die Einstellungen sind getan - jetzt die OpenAI-Libraries\n",
        "print(\"Widgets eingerichtet.\")\n",
        "\n",
        "import tiktoken\n",
        "from openai import OpenAI\n",
        "\n",
        "max_tokens = 256\n",
        "stoptokens = \"###\"\n",
        "\n",
        "def beantworte(temp):\n",
        "    global raetsel_prompt\n",
        "    g_system = ''\n",
        "    g_prompt = raetsel_prompt\n",
        "    previous_messages = []\n",
        "    t = gpt(g_system,\n",
        "            previous_messages,\n",
        "            g_prompt,\n",
        "            g_model = dropdown_model.value,\n",
        "            temperature = temp)\n",
        "    print(\"*\",end=\"\")\n",
        "    return(t)\n",
        "\n",
        "\n",
        "beurteilung_prompt = '''\n",
        "Du bist Datenanalyst. Du liest Antworten auf eine Quizfrage nach der\n",
        "Anzahl der Brüder und Schwestern von Anna. Dabei ist Anna eine der Schwestern.\n",
        "Lies den Antwort-Text und gib die Anzahl der Brüder und Schwestern zurück, die\n",
        "genannt sind. Produziere eine JSON-Datei. Nutze diese Form:\n",
        "{ \"B\":1, \"S\":2 }\n",
        "'''\n",
        "beurteilung_beispiele = [{\"role\": \"user\", \"content\": \"Das Rätsel kann auf den ersten Blick verwirrend erscheinen, aber die Lösung ist eigentlich recht einfach. Wenn Anna drei Brüder hat, dann ist sie selbst eine Schwester für jeden ihrer Brüder. Das bedeutet, dass Anna und ihre drei Brüder insgesamt vier Geschwister sind.\"},\n",
        "                      {\"role\": \"assistant\", \"content\": '{\"B\":3, \"S\":1}'},\n",
        "                      {\"role\": \"user\", \"content\": \"Vier Brüder und Anna und ihre zwei Schwestern.\"},\n",
        "                      {\"role\": \"assistant\", \"content\": '{\"B\":4, \"S\":3}'}]\n",
        "\n",
        "\n",
        "def beurteile(text):\n",
        "    global beurteilung_prompt\n",
        "    global beurteilung_beispiele\n",
        "    global html_spinner\n",
        "    g_system = beurteilung_prompt\n",
        "    previous_messages = beurteilung_beispiele\n",
        "    t = gpt(g_system,\n",
        "            previous_messages,\n",
        "            text,\n",
        "            g_model = \"gpt-3.5-turbo-1106\",\n",
        "            temperature = 0,\n",
        "            json = True)\n",
        "    print(\"+\",end=\"\")\n",
        "    # Gib das JSON als dict zurück\n",
        "    return(t)\n",
        "\n",
        "\n",
        "# Slider für die Temperatur (Default: 1)\n",
        "slider_temperatur = widgets.FloatSlider(\n",
        "    value=1.0,\n",
        "    min=0,\n",
        "    max=2.0,\n",
        "    step=0.1,\n",
        "    description='Temperatur:',\n",
        "    orientation='horizontal',\n",
        "    readout=True\n",
        ")\n",
        "\n",
        "\n",
        "slider_samples = widgets.IntSlider(\n",
        "    value=100,\n",
        "    min=10,\n",
        "    max=500,\n",
        "    step = 10,\n",
        "    description='Durchgänge:',\n",
        "    orientation='horizontal',\n",
        "    readout=True,\n",
        "    layout=widgets.Layout(width='50%')\n",
        ")\n",
        "\n",
        "textbox_max_tokens = widgets.Text(\n",
        "    value='256',\n",
        "    placeholder='0',\n",
        "    description='Max. Antwort:',\n",
        ")\n",
        "\n",
        "text_tokens = widgets.HTML(\n",
        "    value = '<b>Verbrauchte Token</b>: 0 ($0.00)'\n",
        ")\n",
        "\n",
        "dropdown_model = widgets.Dropdown(\n",
        "    # Nimm die oben definierte Preisliste als Basis\n",
        "    options=list(models_token_info.keys()),\n",
        "    value=list(models_token_info.keys())[0],\n",
        "    description='Modell:',\n",
        ")\n",
        "\n",
        "g_prompt = '''Ein Rätsel: Anna hat drei Brüder, jeder hat zwei Schwestern.\n",
        "Wie viele Brüder und Schwestern sind es insgesamt?'''\n",
        "\n",
        "area_raetsel = widgets.Textarea(\n",
        "    value = g_prompt,\n",
        "    rows=10,\n",
        "    description = 'Rätselfrage:',\n",
        "    layout=widgets.Layout(width='80%')\n",
        ")\n",
        "\n",
        "html_samples_tokens = widgets.HTML(\n",
        "    value = f'<b>Samples in Token</b>: {spent_tokens} '\n",
        ")\n",
        "\n",
        "button_start_experiment = widgets.Button(\n",
        "    description='Starten',\n",
        "    layout=widgets.Layout(width='15%'),\n",
        ")\n",
        "\n",
        "# Die Zutaten sind da, jetzt bereite sie zu!\n",
        "\n",
        "def my_main(change):\n",
        "  global temperatur\n",
        "  global num_samples\n",
        "  display(text_tokens)\n",
        "  print(f\"Sammele {num_samples} Antworten\")\n",
        "  html_spinner = widgets.HTML(spinner_html)\n",
        "  display(html_spinner)\n",
        "  antworten = [beantworte(temperatur) for _ in range(num_samples)]\n",
        "  df = pd.DataFrame({\n",
        "      'Text': antworten\n",
        "      })\n",
        "  html_spinner.close()\n",
        "  print()\n",
        "  print(f\"Beurteile die {num_samples} Antworten\")\n",
        "  # GPT3.5 ein Urteil fällen lassen\n",
        "  html_spinner = widgets.HTML(spinner_html)\n",
        "  display(html_spinner)\n",
        "  bewertungen = (df['Text'].apply(beurteile))\n",
        "  html_spinner.close()\n",
        "  df['BS'] = bewertungen.apply(json.loads)\n",
        "\n",
        "  # GPT gibt ein JSON zurück; Rückgabewert der beurteilen() Funktion ist ein dict\n",
        "  # Spalte es in zwei Einzelwerte auf\n",
        "  df['B'] = df['BS'].apply(lambda x: x['B'])\n",
        "  df['S'] = df['BS'].apply(lambda x: x['S'])\n",
        "\n",
        "  # dict-Spalte rausschmeißen, neue Spalte erzeugen, die Summe erzeugt und prüft\n",
        "  df.drop('BS', axis=1, inplace=True)\n",
        "  df['G'] = df['B'] + df['S']\n",
        "  df['OK'] = df.apply(lambda row: row['B'] == 3 and row['S'] == 2, axis=1)\n",
        "\n",
        "  fname = f\"./raetselantworten_{dropdown_model.value}_{temperatur:.1f}.xlsx\"\n",
        "  df.to_excel(fname, index=False)\n",
        "  # Datei herunterladen\n",
        "  files.download(fname)\n",
        "\n",
        "  # Auswertung\n",
        "  print()\n",
        "  print(f\"--- AUSWERTUNG FÜR {dropdown_model.value} mit Temperatur {temperatur:.1f}---\")\n",
        "  prozent_richtig = df['OK'].sum() / num_samples\n",
        "  print(f\"Richtig beantwortet: {prozent_richtig * 100}%\")\n",
        "  print(\"Brüder: \")\n",
        "  print(f\" Median: {df['B'].median()}   Mittelwert: {df['B'].mean():.2f}   StdAbw: {df['B'].std():.2f}\")\n",
        "  print(\"Schwestern: \")\n",
        "  print(f\" Median: {df['S'].median()}   Mittelwert: {df['S'].mean():.2f}   StdAbw: {df['S'].std():.2f}\")\n",
        "  print(\"Geschwister gesamt: \")\n",
        "  print(f\" Median: {df['G'].median()}   Mittelwert: {df['G'].mean():.2f}   StdAbw: {df['G'].std():.2f}\")\n",
        "  df['text_truncated'] = df['Text'].str[:50]\n",
        "  top_drei_df = df['text_truncated'].value_counts().head(3).reset_index()\n",
        "  top_drei_df.columns = ['Text', 'Count']\n",
        "  print(\"Häufigste drei Antworten: \")\n",
        "  for index, row in top_drei_df.iterrows():\n",
        "      print(f\"{row['Text']}   {row['Count'] / num_samples * 100:.1f}%\")\n",
        "  print(\"Done!\")\n",
        "\n",
        "def update_params(change):\n",
        "    global temperatur\n",
        "    global max_tokens\n",
        "    global model\n",
        "    global stoptokens\n",
        "    global raetsel_prompt\n",
        "    global num_samples\n",
        "    global token_limit_reached\n",
        "    # Token-Obergrenze umrechnen\n",
        "    try:\n",
        "        max_tokens = int(textbox_max_tokens.value)\n",
        "        if max_tokens == 0:\n",
        "            max_tokens = None\n",
        "    except ValueError:\n",
        "        max_tokens = None\n",
        "    textbox_max_tokens.value = f'{max_tokens}'\n",
        "    raetsel_prompt = area_raetsel.value\n",
        "    model = dropdown_model.value\n",
        "    if (stoptokens == \"\"):\n",
        "      stoptokens = None\n",
        "    temperatur = slider_temperatur.value\n",
        "    num_samples = slider_samples.value\n",
        "    sample_tokens = calculate_tokens(raetsel_prompt) * num_samples + calculate_tokens(beurteilung_prompt) * num_samples\n",
        "    html_samples_tokens.value = f'Overhead: {sample_tokens} entsprechend $ {input_pricing(sample_tokens):.2f}'\n",
        "\n",
        "# Verbinde die Widgets mit der Funktion zur Verarbeitung der Werte\n",
        "slider_temperatur.observe(update_params, 'value')\n",
        "slider_samples.observe(update_params, 'value')\n",
        "dropdown_model.observe(update_params, 'value')\n",
        "area_raetsel.observe(update_params, 'value')\n",
        "\n",
        "button_start_experiment.on_click(my_main)\n",
        "\n",
        "# Einmal alles berechnen...\n",
        "update_params(0)\n",
        "\n",
        "# ...dann Einstell-Widgets anzeigen\n",
        "display(slider_temperatur,\n",
        "        slider_samples,\n",
        "        dropdown_model,\n",
        "        textbox_max_tokens,\n",
        "        area_raetsel,\n",
        "        html_samples_tokens,\n",
        "        button_start_experiment,\n",
        "        )\n"
      ],
      "metadata": {
        "id": "7A0rpaKpfGXx",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Was die Einstellwerte bedeuten:\n",
        "\n",
        "* **Temperatur:** Wie stark soll der Zufall das Sprachmodell beeinflussen? (0 = wenig, 1 = Normaleinstellung, 1,5 = Irrsinnsgrenze, ab der das Modell delirisch wird)\n",
        "* **Durchgänge:** Wie oft soll die Rätselfrage dem Modell gestellt und von ihm neu beantwortet werden?\n",
        "* **Modell:** Welches Sprachmodell soll befragt werden? Standard ist ```GPT-3.5-turbo-1106```, das (im Jan. 2024) neuesten Update des Sprachmodells, das auch ChatGPT antreibt. Außerdem steht die neue Variante des mächtigeren Modells ```GPT-4``` zur Auswahl - wenn der API-Key es zulässt - und zum Vergleich eine GPT3.5-Variante aus dem Juni 2023, ```gpt-3.5-turbo-0613```\n",
        "* **Max. Antwort:** Wie lang darf die Antwort des Sprachmodells werden? Gerade, wenn der Zufall das Modell zum Schwafeln bringt, ist diese Begrenzung nötig - sonst hört es nicht mehr auf. Die Standardeinstellung von 256 Tokens - entspricht im Deutschen etwa 180 Wörtern - reicht für eine korrekte Antwort dicke aus. Wer keine Längenbegrenzung haben will, stellt 0 ein.\n",
        "* **Rätselfrage:** Das ist der Prompt, den das Sprachmodell beantworten muss. Es bekommt kein Systemprompt dazu, ist also in der Standardrolle eines bemühten und hilfreichen Assistenten.\n",
        "\n",
        "Der **Overhead** gibt an, wie viele Tokens erzeugt werden, um dem Sprachmodell die Frage so oft wie vorgegeben stellen zu können - und die Antworten zu bewerten. Er erlaubt eine erste Abschätzung der Kosten - allerdings kommt noch ein Mehrfaches des Overheads für die erzeugten Antworten hinzu (die noch nicht berechnet werden können).\n",
        "\n",
        "Sobald das Experiment gestartet ist, rechnet das Programm die laufenden Kosten auf. Sie sind für GPT-4 10x so hoch wie für das weniger mächtige Sprachmodell GPT-3.5"
      ],
      "metadata": {
        "id": "nKULJbzG1cjf"
      }
    }
  ]
}
